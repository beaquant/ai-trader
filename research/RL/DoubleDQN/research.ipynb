{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN Trader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helpers import get_stock_code_and_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_space = 3\n",
    "a_space = 3\n",
    "learning_rate = 0.003\n",
    "buffer_size = 10000\n",
    "\n",
    "buffer = np.zeros((buffer_size, s_space + 1 + 1 + s_space))\n",
    "buffer_length = 0\n",
    "gamma = 0.9\n",
    "codes = ['600036']\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.placeholder(tf.float32, [None, s_space])\n",
    "s_next = tf.placeholder(tf.float32, [None, s_space])\n",
    "q_next = tf.placeholder(tf.float32, [None, a_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n",
    "\n",
    "Two NN with same structure, 3 densor layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_critic_nn(s, scope):\n",
    "    w_init, b_init = tf.random_normal_initializer(.0, .3), tf.constant_initializer(.1)\n",
    "    with tf.variable_scope(scope):\n",
    "        s_first_dense = tf.layers.dense(s,\n",
    "                                       32,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_initializer=w_init,\n",
    "                                       bias_initializer=b_init)\n",
    "        s_second_dense = tf.layers.dense(s_first_dense,\n",
    "                                        32,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        kernel_initializer=w_init,\n",
    "                                        bias_initializer=b_init)\n",
    "        q = tf.layers.dense(s_second_dense,\n",
    "                            a_space,\n",
    "                            kernel_initializer=w_init,\n",
    "                            bias_initializer=b_init)\n",
    "        return q\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate NN\n",
    "\n",
    "Generate NN with two layer `eval` and `target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_eval = _build_critic_nn(s, 'q_eval')\n",
    "q_target = _build_critic_nn(s_next, 'q_next')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.squared_difference(q_next, q_eval))\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_eval')\n",
    "t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_target')\n",
    "update_q_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Initial tf config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial tf session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_batch():\n",
    "    indices = np.random.choice(buffer_size, size.batch_size)\n",
    "    batch = buffer[indices, :]\n",
    "    s = batch[:, :s_space]\n",
    "    a = batch[:, s_space: self.s_space + 1]\n",
    "    r = batch[:, -s_space - 1: -s_space]\n",
    "    s_next = batch[:, -s_space:]\n",
    "    return s, a, r, s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 1. If buffer length is less than buffer size, return\n",
    "    if buffer_length < self.buffer_size:\n",
    "        return\n",
    "\n",
    "    # 2. Update Q-Target if need\n",
    "    if total_step % update_q_target_step == 0:\n",
    "        session.run(self.update_q_target_op)\n",
    "    \n",
    "    # 3. Get transition bath\n",
    "    s, a, r, s_next = get_transition_batch()\n",
    "\n",
    "    # 4. Calculate q_eval_next.\n",
    "    q_eval_next = self.session.run(self.q_eval, {self.s: s_next})\n",
    "\n",
    "    # 5. Get action indices and make batch indices.\n",
    "    a_indices = np.argmax(q_eval_next, axis=1)\n",
    "    b_indices = np.arange(self.batch_size, dtype=np.int)\n",
    "\n",
    "    # 6. Calculate q_target_next selected by actions.\n",
    "    q_target_next = self.session.run(q_target, {s_next: s_next})\n",
    "    q_target_next_with_a = q_target_next[b_indices, a_indices]\n",
    "\n",
    "    # 7. Calculate labels.\n",
    "    q_eval = session.run(q_eval, {s: s})\n",
    "    q_next = q_eval.copy()\n",
    "    q_next[b_indices, a.astype(np.int)] = r + gamma * q_target_next_with_a\n",
    "\n",
    "    # 8. Calculate loss.\n",
    "    _, critic_loss = session.run([train_op, loss], {s: s, q_next: q_next})\n",
    "\n",
    "    # 9. Increase total step\n",
    "    total_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a455c4504df9>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a455c4504df9>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    return get_stock_code_and_action(codes, a, use_greedy=True, use_prob=True if mode == 'train' else False))\u001b[0m\n\u001b[0m                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def predict(s):\n",
    "    q = session.run(q_eval, {s: s})\n",
    "    a = np.argmax(q)\n",
    "    return get_stock_code_and_action(codes, a, use_greedy=True, use_prob=True if mode == 'train' else False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQAlpha Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rqalpha\n",
    "\n",
    "from rqalpha.api import *\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtest config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"base\": {\n",
    "    \"start_date\": \"2017-01-01\",\n",
    "    \"end_date\": \"2018-01-02\",\n",
    "    \"benchmark\": \"000300.XSHG\",\n",
    "    \"accounts\": {\n",
    "        \"stock\": 100000\n",
    "    }\n",
    "  },\n",
    "  \"extra\": {\n",
    "    \"log_level\": \"warning\",\n",
    "  },\n",
    "  \"mod\": {\n",
    "    \"sys_analyser\": {\n",
    "      \"enabled\": True,\n",
    "      \"plot\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(context):\n",
    "    context.s1 = codes[0]\n",
    "\n",
    "def before_trading(context):\n",
    "    pass\n",
    "\n",
    "def handle_bar(context, bar_dict):\n",
    "    s = process_data(context, bar_dict)\n",
    "\n",
    "    if s is None:\n",
    "        return\n",
    "\n",
    "    c, a, _ = predict(s)\n",
    "    train()\n",
    "    \n",
    "    if a == ActionCode.Buy:\n",
    "        order(context.s1, 0.10)\n",
    "        buy_open(context.s1, 100)\n",
    "\n",
    "    elif a == ActionCode.Sell:\n",
    "        buy_open(context.s1, 0)\n",
    "\n",
    "def stoploss():\n",
    "    pass\n",
    "\n",
    "def after_trading(context):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rqalpha.run_func(init=strategy.init,\n",
    "             before_trading=before_trading,\n",
    "             handle_bar=handle_bar,\n",
    "             after_trading=after_trading,\n",
    "             config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
